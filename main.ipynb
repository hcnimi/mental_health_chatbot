{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mental Health Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as ticker\n",
    "import os\n",
    "import sys\n",
    "print(sys.executable)\n",
    "import wandb\n",
    "import gradio as gr\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "from rank_bm25 import BM25Okapi\n",
    "from openai import OpenAI\n",
    "load_dotenv()\n",
    "NEON_PG_CONNECTION_URL = os.environ['NEON_PG_CONNECTION_URL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing & Insert to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data():\n",
    "    ds1 = load_dataset(\"Amod/mental_health_counseling_conversations\")\n",
    "    ds2 = load_dataset(\"mpingale/mental-health-chat-dataset\")\n",
    "\n",
    "    # Rename columns \"Context\": \"Question\", \"Response\": \"Answer\" of ds1\n",
    "    ds1 = ds1.rename_column(\"Context\", \"Question\")\n",
    "    ds1 = ds1.rename_column(\"Response\", \"Answer\")\n",
    "    ds2 = ds2.remove_columns([\"questionID\", \"questionTitle\", \"questionLink\", \"topic\", \"therapistInfo\", \"therapistURL\", \"upvotes\", \"views\", \"text\"])\n",
    "    ds2 = ds2.rename_column(\"questionText\", \"Question\")\n",
    "    ds2 = ds2.rename_column(\"answerText\", \"Answer\")\n",
    "\n",
    "    # Convert to pandas DataFrame\n",
    "    df1 = ds1['train'].to_pandas()\n",
    "    df2 = ds2['train'].to_pandas()\n",
    "\n",
    "    # Drop duplicates & NAs\n",
    "    df1 = df1.drop_duplicates(subset=[\"Question\", \"Answer\"]).dropna(subset=[\"Question\", \"Answer\"])\n",
    "    df2 = df2.drop_duplicates(subset=[\"Question\", \"Answer\"]).dropna(subset=[\"Question\", \"Answer\"])\n",
    "\n",
    "    # Combine datasets\n",
    "    combined_df = pd.concat([df1, df2])\n",
    "\n",
    "    questions = combined_df['Question'].tolist()\n",
    "    answers = combined_df['Answer'].tolist()\n",
    "\n",
    "    return questions, answers\n",
    "\n",
    "# Connect to the database\n",
    "try:\n",
    "    connection = psycopg2.connect(NEON_PG_CONNECTION_URL)\n",
    "    connection.autocommit = True\n",
    "    print(\"Connected to Neon Postgres!\")\n",
    "except Exception as e:\n",
    "    print(\"Cannot connect to Neon Postgres:\", e)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "questions, answers = preprocess_data()\n",
    "# Vectorization\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "question_vectors = model.encode(questions)\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS mental_health_qa (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        question TEXT,\n",
    "        answer TEXT,\n",
    "        vector FLOAT8[]\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Insert the data into the database\n",
    "for i in range(len(questions)):\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO mental_health_qa (question, answer, vector)\n",
    "        VALUES (%s, %s, %s)\n",
    "    \"\"\", (questions[i], answers[i], question_vectors[i].tolist()))\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query the database and augment responses\n",
    "def query_and_augment(user_query):\n",
    "    # Connect to the database\n",
    "    connection = psycopg2.connect(NEON_PG_CONNECTION_URL)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    cursor.execute(\"SELECT question, answer FROM mental_health_qa\")\n",
    "    records = cursor.fetchall()\n",
    "\n",
    "    questions = [record[0] for record in records]\n",
    "    answers = [record[1] for record in records]\n",
    "\n",
    "    # Close the connection\n",
    "    connection.close()\n",
    "    \n",
    "    # Implement BM25 to find the best match\n",
    "    tokenized_questions = [q.split() for q in questions]\n",
    "    bm25 = BM25Okapi(tokenized_questions)\n",
    "    best_match_index = bm25.get_top_n(user_query.split(), questions, n=1)[0]\n",
    "    # print the top 5 matches\n",
    "    print(bm25.get_top_n(user_query.split(), questions, n=5))\n",
    "    best_answer = answers[questions.index(best_match_index)]\n",
    "\n",
    "    # Generate augmented answer using the pipeline\n",
    "    # prompt = f\"User: {user_query}\\n\\n\\nLimit your knowledge to these related questions only:\\n\\nBot: {best_answer}\\nBot (improved):\"\n",
    "    prompt = f\"User: {user_query}\\n\\n\\nImprove the response from the database:\\n\\nBot: {best_answer}\\nBot (improved):\"\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Answer the user_query based on the best answer.\\n\\n\" + prompt},\n",
    "            {\"role\": \"assistant\", \"content\": best_answer}\n",
    "        ],\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# answer = query_and_augment(\"My grandson's step-mother sends him to school with a pink Barbie backpack as a form of punishment.\")\n",
    "answer = query_and_augment(\"I'm feeling depressed\")\n",
    "print(answer)\n",
    "# Define Gradio Interface\n",
    "# def chatbot_interface(user_query):\n",
    "#     return query_and_augment(user_query)\n",
    "\n",
    "# interface = gr.Interface(fn=chatbot_interface, inputs=\"text\", outputs=\"text\", title=\"Mental Health Chatbot\")\n",
    "# interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:yvd24ua5) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9885a62d3f48debb0add62895ec98c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.006 MB uploaded\\r'), FloatProgress(value=0.2084592145015106, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fancy-sunset-9</strong> at: <a href='https://wandb.ai/hcnimi-deep-atlas/mental-health-chatbot/runs/yvd24ua5' target=\"_blank\">https://wandb.ai/hcnimi-deep-atlas/mental-health-chatbot/runs/yvd24ua5</a><br/> View project at: <a href='https://wandb.ai/hcnimi-deep-atlas/mental-health-chatbot' target=\"_blank\">https://wandb.ai/hcnimi-deep-atlas/mental-health-chatbot</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240823_101636-yvd24ua5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:yvd24ua5). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca10a8d37e9468985c272ee7409a68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011167688433650054, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/hubert_1/dev/DeepAtlas/gpp/mental_health_chatbot/wandb/run-20240823_101654-pimj8tff</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hcnimi-deep-atlas/mental-health-chatbot/runs/pimj8tff' target=\"_blank\">pretty-planet-10</a></strong> to <a href='https://wandb.ai/hcnimi-deep-atlas/mental-health-chatbot' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hcnimi-deep-atlas/mental-health-chatbot' target=\"_blank\">https://wandb.ai/hcnimi-deep-atlas/mental-health-chatbot</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hcnimi-deep-atlas/mental-health-chatbot/runs/pimj8tff' target=\"_blank\">https://wandb.ai/hcnimi-deep-atlas/mental-health-chatbot/runs/pimj8tff</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from transformers.integrations import WandbCallback\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "import wandb\n",
    "import gradio as gr\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Set the environment variable to adjust memory allocation limits\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "# os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"main.ipynb\"\n",
    "\n",
    "# Initialize Weights and Biases\n",
    "wandb.init(project=\"mental-health-chatbot\")\n",
    "\n",
    "# Check if CUDA is available (optional, more relevant for GPU setup)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conv_id': 'hit:0_conv:1', 'situation': 'I remember going to the fireworks with my best friend. There was a lot of people, but it only felt like us in the world.', 'emotion': 'sentimental', 'conversations': [{'content': 'I remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people, we felt like the only people in the world.', 'role': 'user'}, {'content': 'Was this a friend you were in love with, or just a best friend?', 'role': 'assistant'}, {'content': 'This was a best friend. I miss her.', 'role': 'user'}, {'content': 'Where has she gone?', 'role': 'assistant'}, {'content': 'We no longer talk.', 'role': 'user'}, {'content': 'Oh was this something that happened because of an argument?', 'role': 'assistant'}]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d4f9830a20424f820e08a714c5c0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n",
    "\n",
    "# Ensure pad token is set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the dataset and limit to 10,000 samples\n",
    "dataset = load_dataset('Estwld/empathetic_dialogues_llm', split='train[:10000]')\n",
    "print(dataset[0])\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    concatenated_texts = [\n",
    "        f\"Emotion: {emotion} Situation: {situation} \" + \" \".join([conv['content'] for conv in conversations])\n",
    "        for emotion, situation, conversations in zip(examples['emotion'], examples['situation'], examples['conversations'])\n",
    "    ]\n",
    "    tokenized_inputs = tokenizer(concatenated_texts, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_val_dataset = tokenized_datasets.train_test_split(test_size=0.1)\n",
    "train_dataset = train_val_dataset['train']\n",
    "val_dataset = train_val_dataset['test']\n",
    "\n",
    "train_val_split = train_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_val_split['train']\n",
    "val_dataset = train_val_split['test']\n",
    "test_dataset = val_dataset.train_test_split(test_size=0.5)['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    bf16=True,  # Mixed precision training\n",
    "    gradient_accumulation_steps=16,  # Gradient accumulation\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"phi-2-finetuning\"\n",
    ")\n",
    "\n",
    "# Define metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    accuracy = load_metric(\"accuracy\").compute(predictions=predictions, references=labels)\n",
    "    bleu = load_metric(\"bleu\").compute(predictions=predictions, references=labels)\n",
    "    perplexity = load_metric(\"perplexity\").compute(predictions=predictions, references=labels)\n",
    "    f1 = load_metric(\"f1\").compute(predictions=predictions, references=labels)\n",
    "    return {\"accuracy\": accuracy, \"bleu\": bleu, \"perplexity\": perplexity, \"f1\": f1}\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# Define prediction function\n",
    "def predict(input_text):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    outputs = model.generate(**inputs)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3690b295cdd84edda227cf6842f3938d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/759 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.699, 'grad_norm': 7.526485919952393, 'learning_rate': 4.9341238471673256e-05, 'epoch': 0.04}\n",
      "{'loss': 5.2732, 'grad_norm': 14.982749938964844, 'learning_rate': 4.868247694334651e-05, 'epoch': 0.08}\n",
      "{'loss': 3.0443, 'grad_norm': 13.900446891784668, 'learning_rate': 4.8023715415019764e-05, 'epoch': 0.12}\n",
      "{'loss': 1.2564, 'grad_norm': 1.8573697805404663, 'learning_rate': 4.736495388669302e-05, 'epoch': 0.16}\n",
      "{'loss': 0.7184, 'grad_norm': 0.38899242877960205, 'learning_rate': 4.670619235836627e-05, 'epoch': 0.2}\n",
      "{'loss': 0.6355, 'grad_norm': 0.24211318790912628, 'learning_rate': 4.6047430830039526e-05, 'epoch': 0.24}\n",
      "{'loss': 0.5985, 'grad_norm': 0.17091038823127747, 'learning_rate': 4.538866930171278e-05, 'epoch': 0.28}\n",
      "{'loss': 0.5644, 'grad_norm': 0.15374329686164856, 'learning_rate': 4.472990777338604e-05, 'epoch': 0.32}\n"
     ]
    }
   ],
   "source": [
    "# Clear the cache\n",
    "torch.mps.empty_cache()\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate(test_dataset)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"Phi-2 Fine-Tuned Model\",\n",
    "    description=\"Enter text to get predictions from the fine-tuned Phi-2 model.\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
