{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mental Health Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as ticker\n",
    "import os\n",
    "import wandb\n",
    "import gradio as gr\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "from rank_bm25 import BM25Okapi\n",
    "from openai import OpenAI\n",
    "load_dotenv()\n",
    "NEON_PG_CONNECTION_URL = os.environ['NEON_PG_CONNECTION_URL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing & Insert to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Neon Postgres!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hubert_1/.pyenv/versions/3.11.9/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data():\n",
    "    ds1 = load_dataset(\"Amod/mental_health_counseling_conversations\")\n",
    "    ds2 = load_dataset(\"mpingale/mental-health-chat-dataset\")\n",
    "\n",
    "    # Rename columns \"Context\": \"Question\", \"Response\": \"Answer\" of ds1\n",
    "    ds1 = ds1.rename_column(\"Context\", \"Question\")\n",
    "    ds1 = ds1.rename_column(\"Response\", \"Answer\")\n",
    "    ds2 = ds2.remove_columns([\"questionID\", \"questionTitle\", \"questionLink\", \"topic\", \"therapistInfo\", \"therapistURL\", \"upvotes\", \"views\", \"text\"])\n",
    "    ds2 = ds2.rename_column(\"questionText\", \"Question\")\n",
    "    ds2 = ds2.rename_column(\"answerText\", \"Answer\")\n",
    "\n",
    "    # Convert to pandas DataFrame\n",
    "    df1 = ds1['train'].to_pandas()\n",
    "    df2 = ds2['train'].to_pandas()\n",
    "\n",
    "    # Drop duplicates & NAs\n",
    "    df1 = df1.drop_duplicates(subset=[\"Question\", \"Answer\"]).dropna(subset=[\"Question\", \"Answer\"])\n",
    "    df2 = df2.drop_duplicates(subset=[\"Question\", \"Answer\"]).dropna(subset=[\"Question\", \"Answer\"])\n",
    "\n",
    "    # Combine datasets\n",
    "    combined_df = pd.concat([df1, df2])\n",
    "\n",
    "    questions = combined_df['Question'].tolist()\n",
    "    answers = combined_df['Answer'].tolist()\n",
    "\n",
    "    return questions, answers\n",
    "\n",
    "# Connect to the database\n",
    "try:\n",
    "    connection = psycopg2.connect(NEON_PG_CONNECTION_URL)\n",
    "    connection.autocommit = True\n",
    "    print(\"Connected to Neon Postgres!\")\n",
    "except Exception as e:\n",
    "    print(\"Cannot connect to Neon Postgres:\", e)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "questions, answers = preprocess_data()\n",
    "# Vectorization\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "question_vectors = model.encode(questions)\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS mental_health_qa (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        question TEXT,\n",
    "        answer TEXT,\n",
    "        vector FLOAT8[]\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Insert the data into the database\n",
    "for i in range(len(questions)):\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO mental_health_qa (question, answer, vector)\n",
    "        VALUES (%s, %s, %s)\n",
    "    \"\"\", (questions[i], answers[i], question_vectors[i].tolist()))\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: User: what's depression?\n",
      "\n",
      "\n",
      "Limit your knowledge to these related questions only:\n",
      "\n",
      "Bot: Probably.  I always tell the new parents I work with (and this is just as true for dads as it is for moms) that if you are feeling off, seek help.  Postpartum Support International (www.postpartum.net) is a great place to start finding resources and locating support.\n",
      "Bot (improved):\n"
     ]
    }
   ],
   "source": [
    "# Function to query the database and augment responses\n",
    "def query_and_augment(user_query):\n",
    "    # Connect to the database\n",
    "    connection = psycopg2.connect(NEON_PG_CONNECTION_URL)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    cursor.execute(\"SELECT question, answer FROM mental_health_qa\")\n",
    "    records = cursor.fetchall()\n",
    "\n",
    "    questions = [record[0] for record in records]\n",
    "    answers = [record[1] for record in records]\n",
    "\n",
    "    # Close the connection\n",
    "    connection.close()\n",
    "\n",
    "    # Implement BM25 to find the best match\n",
    "    tokenized_questions = [q.split() for q in questions]\n",
    "    bm25 = BM25Okapi(tokenized_questions)\n",
    "    best_match_index = bm25.get_top_n(user_query.split(), questions, n=1)[0]\n",
    "    best_answer = answers[questions.index(best_match_index)]\n",
    "\n",
    "    # Generate augmented answer using the pipeline\n",
    "    prompt = f\"User: {user_query}\\n\\n\\nLimit your knowledge to these related questions only:\\n\\nBot: {best_answer}\\nBot (improved):\"\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Answer the user_query based on the best answer\"},\n",
    "            {\"role\": \"assistant\", \"content\": best_answer}\n",
    "        ],\n",
    "        model=\"gpt-4o\",\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "# Define Gradio Interface\n",
    "def chatbot_interface(user_query):\n",
    "    return query_and_augment(user_query)\n",
    "\n",
    "interface = gr.Interface(fn=chatbot_interface, inputs=\"text\", outputs=\"text\", title=\"Mental Health Chatbot\")\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the text generation pipeline\n",
    "# generator = pipeline(\"text-generation\", model=\"Llama-3-8B-instruct\")\n",
    "\n",
    "# # Function to query the database and augment responses\n",
    "# def query_and_augment(user_query):\n",
    "#     # Connect to the database\n",
    "#     connection = psycopg2.connect(NEON_PG_CONNECTION_URL)\n",
    "#     cursor = connection.cursor()\n",
    "\n",
    "#     cursor.execute(\"SELECT question, answer FROM mental_health_chatbot\")\n",
    "#     records = cursor.fetchall()\n",
    "\n",
    "#     questions = [record[0] for record in records]\n",
    "#     answers = [record[1] for record in records]\n",
    "\n",
    "#     # Close the connection\n",
    "#     connection.close()\n",
    "\n",
    "#     # Implement BM25 to find the best match\n",
    "#     tokenized_questions = [q.split() for q in questions]\n",
    "#     bm25 = BM25Okapi(tokenized_questions)\n",
    "#     best_match_index = bm25.get_top_n(user_query.split(), questions, n=1)[0]\n",
    "#     best_answer = answers[questions.index(best_match_index)]\n",
    "\n",
    "#     # Generate augmented answer using the pipeline\n",
    "#     prompt = f\"User: {user_query}\\nBot: {best_answer}\\nBot (improved):\"\n",
    "#     augmented_answer = generator(prompt, max_new_tokens=100, return_full_text=False)[0]['generated_text']\n",
    "\n",
    "#     return augmented_answer\n",
    "\n",
    "# # Define Gradio Interface\n",
    "# def chatbot_interface(user_query):\n",
    "#     return query_and_augment(user_query)\n",
    "\n",
    "# interface = gr.Interface(fn=chatbot_interface, inputs=\"text\", outputs=\"text\", title=\"Mental Health Chatbot\")\n",
    "# interface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
